---
layout: post
title:  "Coursera Machine Learning : W3"
categories: technology
tags: coursera cs229 
author: LZN
---

### W3

#### Notes

##### Classification and Logistic Regression
* Sigmoid function -- Logistic function `1/(1+e^(-z))`
* Logistic regression: thetax=1/(1+e^(thetaX)), estimates the probability that y=1 on input x

##### Decision Boundary
* The decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.
* Directly use the cost function of logistic regression will lead to a non-convex function with many local optima.
* New cost function, cost(hthetax, y)=-log(htheta(x)), if y=1; and cost(hthetax,y)= -log(1-htheta(x)), if y=0. This will promise when hthetax gives a wrong answer, a very large penalty will be imposed.
* Single expression: cost(thetax, y)=`-y*log(htheta(x))-(1-y)*log(1-htheta(x))`
* Advantages: 1. Maximum likelihood estimation 2. Convex
* The form of gradient decent of logistic regression is the same as linear regression, just the hypothesis changes.
*

