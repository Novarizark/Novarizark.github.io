---
layout: post
title:  "【COAP】大气科学类文献语料库建立" 
categories: project
tags: web project coap corpus python
author: LZN
---

* content
{:toc}

背单词找方法<a href="http://www.zhihu.com/question/26814125/answer/47549326">刷知乎</a>的时候接触到了corpus（语料库）的概念，昨晚睡不着随便刷知乎，看到了<a href="http://zhuanlan.zhihu.com/talk-about-english/20044356">这篇回答</a>，也是利用语料库统计词频的方法，筛选出《经济学人》中专业词汇，从而有针对性地进行记忆，高效快速地训练自己能够读懂相关文章的能力。而作者没有局限于此，又对统计出的词频做了数据挖掘和数据分析，比如近几年经济学人对中国关注度的变化，对中国哪家互联网企业关注最多等等。看到这些，突然眼前一亮，为什么我们不能建立一个专业期刊的语料库，然后进行数据挖掘和分析呢？这下更睡不着了，决定取名为COAP (Corpus of Atmospheric Papers)。简单构思实现方法，认为这是可行的：
<ol>
	<li>首先我们需要将指定期刊所有文章（带摘要的引文信息也可以，与paperhub联系），爬下来。</li>
	<li>写词频统计程序和分析程序，对语料库进行整理分析。</li>
</ol>
可能遇到的困难:

1. 期刊方自我保护：谁都不希望有人把自己所有的家当爬走，很可能会对下载进行限制。比如频率（这个可以通过延时解决）、数量（如果限制IP，没办法，只能换）、地址加密（这个也比较麻烦，所以需要学习爬虫）等等。今天简单看了JC的情况，发现地址虽然进行了加密，但是用爬虫应该还是能解决问题。

2. 语义分析、正则表达式一直是弱项，这点需要额外加强，另外目前对语料库总数据量还不能确定（10-1000GB）区间吧，真正分析起来时间成本也是个问题，多线程需要考虑。

上述可能遇到的问题并非不能解决，简单设想，最好的工具就是python了，正好以此项目作为契机，入门python。一些资料留在这里，希望今年能把这个事情做出来！

<a href="http://www.zhihu.com/question/20899988">【知乎】如何入门python爬虫</a>

<a href="http://www.cnblogs.com/fnng/p/3576154.html">python实现简单爬虫功能</a>
